{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Из [статьи](https://www.researchgate.net/publication/332242485_Angular_Softmax_Loss_for_End-to-end_Speaker_Verification) \"angular softmax speaker verification\" и еще [одной статьи](https://deepai.org/publication/deep-speaker-embeddings-for-far-field-speaker-recognition-on-short-utterances) \"Deep Speaker Embeddings for Speaker Recognition\" взяты параметры для достижения цели этого проекта:\n",
    "    features: MFFC - n_mel = 23, frame_length = 25ms, duration treck = 3s.\n",
    "    loss: AMSoftmax(s=3,m-0.35), ArcFace loss, ASoftmax(m = 3)\n",
    "    backend: cosine similarity\n",
    "\n",
    "Use layer TimeDistributed from [article](https://ru-keras.com/wrappers-sloi/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#libs\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from loss_layers import AMSoftmax, ArcFace\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "from sklearn.model_selection import train_test_split\n",
    "from DataGen import DataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Conv1D, Conv2D, LSTM, Dropout,LeakyReLU, PReLU, Reshape, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D, Flatten, concatenate, GlobalAveragePooling1D, AveragePooling1D, Lambda, PReLU, GRU, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers, initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def amsoftmax_loss(y_true, y_pred):\n",
    "    scale=30.0\n",
    "    margin=0.25\n",
    "\n",
    "    label = tf.reshape(tf.argmax(y_true, axis=-1), shape=(-1,1))\n",
    "    label = tf.cast(label, dtype=tf.int32) # y\n",
    "    batch_range = tf.reshape(tf.range(tf.shape(y_pred)[0]),shape=(-1,1)) # 0~batchsize-1\n",
    "    indices_of_groundtruth = tf.concat([batch_range, tf.reshape(label,shape=(-1,1))], axis=1) # 2columns vector, 0~batchsize-1 and label\n",
    "    groundtruth_score = tf.gather_nd(y_pred, indices_of_groundtruth) # score of groundtruth\n",
    "\n",
    "    m = tf.constant(margin,name='m')\n",
    "    s = tf.constant(scale,name='s')\n",
    "\n",
    "    added_margin = tf.cast(tf.greater(groundtruth_score,m),dtype=tf.float32)*m # if groundtruth_score>m, groundtruth_score-m\n",
    "    added_margin = tf.reshape(added_margin,shape=(-1,1))\n",
    "    added_embeddingFeature = tf.subtract(y_pred, y_true*added_margin)*s # s(cos_theta_yi-m), s(cos_theta_j)\n",
    "\n",
    "    cross_ent = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=added_embeddingFeature)\n",
    "    loss = tf.reduce_mean(cross_ent)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37776 9445\n"
     ]
    }
   ],
   "source": [
    "path = Path('C:\\\\datasets_from_D\\\\train_vox2_2half\\\\wav_vad\\\\0\\\\')\n",
    "all_audio = natsorted(list(path.rglob('*.wav')))\n",
    "# print(audio[:150])\n",
    "train_audio, test_audio = train_test_split(all_audio, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(len(train_audio), len(test_audio))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepaire generic data for train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght train_gen 2276 batches\n"
     ]
    }
   ],
   "source": [
    "# data = DataGenerator(train_audio[:20])\n",
    "# d, cl = next(iter(data))\n",
    "# print(d.shape, cl)\n",
    "train_gen = DataGenerator(train_audio)\n",
    "val_gen = DataGenerator(test_audio)\n",
    "print(f'Lenght train_gen {len(train_gen)} batches')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model from article \"angular softmax speaker verification\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# #Если не  с работает: 1 -  применить рекурентные слои, 2 - применить Conv2D, 3 - изменить Amsoftmax layer на Arclayer, 4- Применить amsoftmax loss а не слой.\n",
    "# from tensorflow.keras import regularizers\n",
    "# def create(input_shape=(301, 23)):\n",
    "#     input_1 = Input(shape=input_shape)\n",
    "#\n",
    "#     #features block\n",
    "#     #1\n",
    "#     x = Conv1D(23, kernel_size=5, name='Conv_1')(input_1)\n",
    "#     x = PReLU()(x)\n",
    "#     x = BatchNormalization(momentum=0.99, name='BN_1')(x)\n",
    "#     x = MaxPooling1D(pool_size=3, name='MP_1')(x)\n",
    "#     # x = Conv1D(128, kernel_size=3, activation='relu')(x) kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)\n",
    "#     # x = Conv1D(256, kernel_size=3, activation='relu')(x)\n",
    "#     # x = LSTM(128, return_sequences=True)(x)\n",
    "#     x = TimeDistributed(Dense(23, activation='relu', name=\"TD_1\"))(x)\n",
    "#     # x = BatchNormalization(momentum=0.99, name='BN_1')(x)\n",
    "#\n",
    "#     #2\n",
    "#     x = Conv1D(512, 3, activation='relu', name='Conv_2')(x)\n",
    "#     x = PReLU()(x)\n",
    "#     x = BatchNormalization(momentum=0.95, name='BN_2')(x)\n",
    "#     x = MaxPooling1D(pool_size=3, name='MP_2')(x)\n",
    "#     #x = TimeDistributed(Dense(512, activation='relu', name=\"TD_2\"))(x)\n",
    "#     # x = BatchNormalization(momentum=0.95, name='BN_2')(x)\n",
    "#\n",
    "#     #3\n",
    "#     x = Conv1D(512, 3, activation='relu', name='Conv_3')(x)\n",
    "#     x = PReLU()(x)\n",
    "#     x = BatchNormalization(momentum=0.99, name='BN_3')(x)\n",
    "#     x = MaxPooling1D(pool_size=3, name='MP_3')(x)\n",
    "#     #x = TimeDistributed(Dense(512, activation='relu', name=\"TD_3\"))(x)\n",
    "#     # x = BatchNormalization(momentum=0.99, name='BN_3')(x)\n",
    "#\n",
    "#     #4\n",
    "#     x = Conv1D(512, 1, activation='relu', name='Conv_4')(x)\n",
    "#     x = PReLU()(x)\n",
    "#     x = BatchNormalization(momentum=0.95, name='BN_4')(x)\n",
    "#     x = MaxPooling1D(pool_size=3, name='MP_4')(x)\n",
    "#     #x = TimeDistributed(Dense(512, activation='relu', name=\"TD_4\"))(x)\n",
    "#     # x = BatchNormalization(momentum=0.95, name='BN_4')(x)\n",
    "#\n",
    "#     #5\n",
    "#     x = Conv1D(1500, 1, activation='relu', name='Conv_5')(x)\n",
    "#     x = PReLU()(x)\n",
    "#     # x = TimeDistributed(Dense(1500, activation='relu', name=\"TD_5\"))(x)\n",
    "#     #x = BatchNormalization(momentum=0.99, name='BN_5')(x)\n",
    "#\n",
    "#     #Statistic block\n",
    "#     x = LayerNormalization(name='LNorm')(x) # layerNorm with mean = 0, std = 1, standardization before averaging\n",
    "#     x = AveragePooling1D(name='AVGP')(x)\n",
    "#\n",
    "#     #Recurent layers block\n",
    "#     x = GRU(256,recurrent_activation=None,return_sequences=True)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = GRU(256,recurrent_activation=None,return_sequences=False)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = PReLU()(x)\n",
    "#\n",
    "#     # Classification block\n",
    "#     # x = Flatten(name='Flt')(x)\n",
    "#     x = Dropout(0.5, name='Drop_1')(x)\n",
    "#     x = Dense(3000, activation='relu', name='D_1')(x)\n",
    "#     x = BatchNormalization(momentum=0.95, name='BN_6')(x)\n",
    "#     # x = Dense(512, activation='relu', use_bias=False, name='D_2')(x)\n",
    "#     # x = BatchNormalization(momentum=0.95, name='BN_7')(x)\n",
    "#     x = Lambda(lambda c: K.l2_normalize(c, axis=-1))(x) # l2 normalization ?\n",
    "#     embed = Dense(300, activation='relu', name='embed')(x)\n",
    "#     outputs = Dense(1000, activation='softmax')(embed)  #  AMSoftmax(1000, s=35, m=0.63, name='AMSoftmax')(embed)    Dense(1000, activation='softmax')(embed)\n",
    "#\n",
    "#     out_model = Model(input_1, outputs)\n",
    "#     out_model.compile(loss=amsoftmax_loss, optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "#\n",
    "#     return out_model\n",
    "#\n",
    "# model = create()\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 301, 23)]    0           []                               \n",
      "                                                                                                  \n",
      " Conv_1 (Conv1D)                (None, 297, 23)      2668        ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " p_re_lu_3 (PReLU)              (None, 297, 23)      6831        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " BN_1 (BatchNormalization)      (None, 297, 23)      92          ['p_re_lu_3[0][0]']              \n",
      "                                                                                                  \n",
      " time_distributed_12 (TimeDistr  (None, 297, 23)     552         ['BN_1[0][0]']                   \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " BN_2 (BatchNormalization)      (None, 297, 23)      92          ['time_distributed_12[0][0]']    \n",
      "                                                                                                  \n",
      " time_distributed_13 (TimeDistr  (None, 297, 512)    12288       ['BN_2[0][0]']                   \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " time_distributed_14 (TimeDistr  (None, 297, 1500)   769500      ['time_distributed_13[0][0]']    \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " BN_3 (BatchNormalization)      (None, 297, 1500)    6000        ['time_distributed_14[0][0]']    \n",
      "                                                                                                  \n",
      " time_distributed_15 (TimeDistr  (None, 297, 3000)   4503000     ['BN_3[0][0]']                   \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 297, 3000)    0           ['time_distributed_15[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 297, 3000)   6000        ['leaky_re_lu_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " GLAB_1 (GlobalAveragePooling1D  (None, 3000)        0           ['layer_normalization_3[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " D_1 (Dense)                    (None, 3000)         9003000     ['GLAB_1[0][0]']                 \n",
      "                                                                                                  \n",
      " Drop_1 (Dropout)               (None, 3000)         0           ['D_1[0][0]']                    \n",
      "                                                                                                  \n",
      " BN_7 (BatchNormalization)      (None, 3000)         12000       ['Drop_1[0][0]']                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 3000)         0           ['BN_7[0][0]']                   \n",
      "                                                                                                  \n",
      " embed (Dense)                  (None, 300)          900300      ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " arc_face_2 (ArcFace)           (None, 1000)         300000      ['embed[0][0]',                  \n",
      "                                                                  'input_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,522,323\n",
      "Trainable params: 15,513,231\n",
      "Non-trainable params: 9,092\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create(input_shape=(301, 23)):\n",
    "    input_1 = Input(shape=input_shape)\n",
    "    label = Input(shape=(1000,))\n",
    "\n",
    "    #features block\n",
    "    #Conv1D_ 1\n",
    "    x = Conv1D(23, kernel_size=5, name='Conv_1')(input_1)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.99, name='BN_1')(x)\n",
    "    #x = MaxPooling1D(pool_size=3, name='MP_1')(x)\n",
    "\n",
    "    x = TimeDistributed(Dense(23, activation='relu', name=\"TD_1\"))(x)\n",
    "    x = BatchNormalization(momentum=0.99, name='BN_2')(x)\n",
    "    x = TimeDistributed(Dense(512, activation='relu', name=\"TD_1\"))(x)\n",
    "    x = TimeDistributed(Dense(1500, activation='relu', name=\"TD_1\"))(x)\n",
    "    x = BatchNormalization(momentum=0.99, name='BN_3')(x)\n",
    "    x = TimeDistributed(Dense(3000, activation='relu', name=\"TD_1\"))(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    #Average block\n",
    "    x = LayerNormalization()(x)\n",
    "    x = GlobalAveragePooling1D( name='GLAB_1')(x)\n",
    "\n",
    "\n",
    "    #Classific block\n",
    "\n",
    "    # x = Flatten(name='Flt')(x)\n",
    "    x = Dense(3000, activation='relu', name='D_1')(x)\n",
    "    x = Dropout(0.3, name='Drop_1')(x)\n",
    "    # x = Dense(500, activation='relu', name='D_2')(x)\n",
    "    x = BatchNormalization(momentum=0.95, name='BN_7')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    embed = Dense(300, name='embed')(x)\n",
    "\n",
    "    #output with custom layer\n",
    "    #norm_emb = BatchNormalization(momentum=0.95, name='BN_6')(embed)\n",
    "    outputs = ArcFace(n_classes=1000)([embed, label])  # AMSoftmax(1000, s=30, m=0.63, name = 'AMSoftmax')(norm_emb)\n",
    "\n",
    "    model = Model([input_1, label], outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "my_model = create()\n",
    "my_model.summary()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#Example TimeDistribute\n",
    "# inputs = tf.keras.Input(shape=(10, 128, 128, 3))\n",
    "# conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))\n",
    "# outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)\n",
    "# outputs.shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Callbaxk**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sv_mod = ModelCheckpoint(\n",
    "    filepath = 'D:\\\\Andrey\\\\data\\\\weights\\\\ARC_mffc_img\\\\\\\n",
    "model_TDL\\\\modelTD_b32_301_23_0f_ohe_{epoch:02d}-loss-{loss:.4f}_val_los-{val_loss:.4f}.hdf5',\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    save_best_only = True\n",
    ")\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                            patience=6,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.9,\n",
    "                                            min_lr=0.00001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**train**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2276/2276 [==============================] - 612s 262ms/step - loss: 29.9143 - accuracy: 0.0015 - val_loss: 29.9451 - val_accuracy: 0.0017 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "2276/2276 [==============================] - 604s 265ms/step - loss: 29.9555 - accuracy: 0.0015 - val_loss: 29.9456 - val_accuracy: 0.0017 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "2276/2276 [==============================] - 606s 266ms/step - loss: 29.9555 - accuracy: 0.0015 - val_loss: 29.9456 - val_accuracy: 0.0017 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "2276/2276 [==============================] - 606s 266ms/step - loss: 29.9555 - accuracy: 0.0015 - val_loss: 29.9448 - val_accuracy: 0.0017 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "2276/2276 [==============================] - 607s 267ms/step - loss: 29.9555 - accuracy: 0.0015 - val_loss: 29.9459 - val_accuracy: 0.0017 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "2276/2276 [==============================] - 610s 268ms/step - loss: 29.9555 - accuracy: 0.0015 - val_loss: 29.9459 - val_accuracy: 0.0017 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "2276/2276 [==============================] - 611s 268ms/step - loss: 29.9555 - accuracy: 0.0015 - val_loss: 29.9456 - val_accuracy: 0.0017 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "2276/2276 [==============================] - 612s 269ms/step - loss: 29.9555 - accuracy: 0.0015 - val_loss: 29.9466 - val_accuracy: 0.0017 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "2276/2276 [==============================] - 585s 257ms/step - loss: 29.9555 - accuracy: 0.0015 - val_loss: 29.9456 - val_accuracy: 0.0017 - lr: 0.0100\n",
      "Epoch 10/100\n",
      " 803/2276 [=========>....................] - ETA: 5:04 - loss: 29.9475 - accuracy: 0.0018"
     ]
    }
   ],
   "source": [
    "history = my_model.fit(train_gen,\n",
    "              epochs = 100,\n",
    "              validation_data = (val_gen),\n",
    "              callbacks = [sv_mod, learning_rate_reduction],\n",
    "              verbose = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#  model.save('D:\\Andrey\\data\\weights\\ARC_mffc_img\\model_TDL\\Model_weights.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#my_model.load_weights('D:\\Andrey\\data\\weights\\ARC_mffc_img\\model_TDL\\modelTD_b32_301_23_0f_ohe_28-loss-6.1041_val_los-6.2124.hdf5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
