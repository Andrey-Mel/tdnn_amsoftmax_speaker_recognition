{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Из [статьи](https://www.researchgate.net/publication/332242485_Angular_Softmax_Loss_for_End-to-end_Speaker_Verification) \"angular softmax speaker verification\" и еще [одной статьи](https://deepai.org/publication/deep-speaker-embeddings-for-far-field-speaker-recognition-on-short-utterances) \"Deep Speaker Embeddings for Speaker Recognition\" взяты параметры для достижения цели этого проекта:\n",
    "    features: MFFC - n_mel = 23, frame_length = 25ms, duration treck = 3s.\n",
    "    loss: AMSoftmax(s=3,m-0.35), ArcFace loss, ASoftmax(m = 3)\n",
    "    backend: cosine similarity\n",
    "\n",
    "Use layer TimeDistributed from [article](https://ru-keras.com/wrappers-sloi/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#libs\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from loss_layers import AMSoftmax, ArcFace\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "from sklearn.model_selection import train_test_split\n",
    "from DataGen import DataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Conv1D, Conv2D, LSTM, Dropout,LeakyReLU, PReLU, Reshape, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D, Flatten, concatenate, GlobalAveragePooling1D, AveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers, initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37776 9445\n"
     ]
    }
   ],
   "source": [
    "path = Path('C:\\\\datasets_from_D\\\\train_vox2_2half\\\\wav_vad\\\\0\\\\')\n",
    "all_audio = natsorted(list(path.rglob('*.wav')))\n",
    "# print(audio[:150])\n",
    "train_audio, test_audio = train_test_split(all_audio, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(len(train_audio), len(test_audio))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepaire generic data for train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# data = DataGenerator(train_audio[:20])\n",
    "# d, cl = next(iter(data))\n",
    "# print(d.shape, cl)\n",
    "train_gen = DataGenerator(train_audio)\n",
    "val_gen = DataGenerator(test_audio)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model from article \"angular softmax speaker verification\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 301, 23)]         0         \n",
      "                                                                 \n",
      " Conv_1 (Conv1D)             (None, 297, 23)           2668      \n",
      "                                                                 \n",
      " MP_1 (MaxPooling1D)         (None, 148, 23)           0         \n",
      "                                                                 \n",
      " time_distributed_40 (TimeDi  (None, 148, 512)         12288     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " BN_1 (BatchNormalization)   (None, 148, 512)          2048      \n",
      "                                                                 \n",
      " Conv_2 (Conv1D)             (None, 146, 512)          786944    \n",
      "                                                                 \n",
      " MP_2 (MaxPooling1D)         (None, 73, 512)           0         \n",
      "                                                                 \n",
      " time_distributed_41 (TimeDi  (None, 73, 512)          262656    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " BN_2 (BatchNormalization)   (None, 73, 512)           2048      \n",
      "                                                                 \n",
      " Conv_3 (Conv1D)             (None, 71, 512)           786944    \n",
      "                                                                 \n",
      " MP_3 (MaxPooling1D)         (None, 35, 512)           0         \n",
      "                                                                 \n",
      " time_distributed_42 (TimeDi  (None, 35, 512)          262656    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " BN_3 (BatchNormalization)   (None, 35, 512)           2048      \n",
      "                                                                 \n",
      " Conv_4 (Conv1D)             (None, 35, 512)           262656    \n",
      "                                                                 \n",
      " MP_4 (MaxPooling1D)         (None, 17, 512)           0         \n",
      "                                                                 \n",
      " time_distributed_43 (TimeDi  (None, 17, 512)          262656    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " BN_4 (BatchNormalization)   (None, 17, 512)           2048      \n",
      "                                                                 \n",
      " Conv_5 (Conv1D)             (None, 17, 1500)          769500    \n",
      "                                                                 \n",
      " time_distributed_44 (TimeDi  (None, 17, 1500)         2251500   \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " BN_5 (BatchNormalization)   (None, 17, 1500)          6000      \n",
      "                                                                 \n",
      " AVGP (AveragePooling1D)     (None, 8, 1500)           0         \n",
      "                                                                 \n",
      " Flt (Flatten)               (None, 12000)             0         \n",
      "                                                                 \n",
      " Drop_1 (Dropout)            (None, 12000)             0         \n",
      "                                                                 \n",
      " D_1 (Dense)                 (None, 3000)              36003000  \n",
      "                                                                 \n",
      " BN_6 (BatchNormalization)   (None, 3000)              12000     \n",
      "                                                                 \n",
      " D_2 (Dense)                 (None, 512)               1536000   \n",
      "                                                                 \n",
      " embed (Dense)               (None, 300)               153600    \n",
      "                                                                 \n",
      " AMSoftmax (AMSoftmax)       (None, 1000)              300000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,679,260\n",
      "Trainable params: 43,666,164\n",
      "Non-trainable params: 13,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Если не  с работает: 1 -  применить рекурентные слои, 2 - применить Conv2D, 3 - изменить Amsoftmax layer на Arclayer, 4- Применить amsoftmax loss а не слой.\n",
    "from tensorflow.keras import regularizers\n",
    "def create(input_shape=(301, 23)):\n",
    "    input_1 = Input(shape=input_shape)\n",
    "\n",
    "\n",
    "    x = Conv1D(23, kernel_size=5, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-4), name='Conv_1')(input_1)\n",
    "    x = MaxPooling1D(pool_size=2, name='MP_1')(x)\n",
    "    # x = Conv1D(128, kernel_size=3, activation='relu')(x) kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)\n",
    "    #\n",
    "    # x = Conv1D(256, kernel_size=3, activation='relu')(x)\n",
    "    # x = LSTM(128, return_sequences=True)(x)\n",
    "    x = TimeDistributed(Dense(512, activation='relu', name=\"TD_1\"))(x)\n",
    "    x = BatchNormalization(momentum=0.99, name='BN_1')(x)\n",
    "    x = Conv1D(512, 3, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-4), name='Conv_2')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='MP_2')(x)\n",
    "    x = TimeDistributed(Dense(512, activation='relu', name=\"TD_2\"))(x)\n",
    "    x = BatchNormalization(momentum=0.95, name='BN_2')(x)\n",
    "    x = Conv1D(512, 3, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-4), name='Conv_3')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='MP_3')(x)\n",
    "    x = TimeDistributed(Dense(512, activation='relu', name=\"TD_3\"))(x)\n",
    "    x = BatchNormalization(momentum=0.99, name='BN_3')(x)\n",
    "    x = Conv1D(512, 1, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-4), name='Conv_4')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='MP_4')(x)\n",
    "    x = TimeDistributed(Dense(512, activation='relu', name=\"TD_4\"))(x)\n",
    "    x = BatchNormalization(momentum=0.95, name='BN_4')(x)\n",
    "    x = Conv1D(1500, 1, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-4), name='Conv_5')(x)\n",
    "    x = TimeDistributed(Dense(1500, activation='relu', name=\"TD_5\"))(x)\n",
    "    x = BatchNormalization(momentum=0.99, name='BN_5')(x)\n",
    "    x = AveragePooling1D(name='AVGP')(x)\n",
    "    x = Flatten(name='Flt')(x)\n",
    "    x = Dropout(0.5, name='Drop_1')(x)\n",
    "    x = Dense(3000, activation='relu', kernel_regularizer=regularizers.L2(l2=1e-4), name='D_1')(x)\n",
    "    x = BatchNormalization(momentum=0.95, name='BN_6')(x)\n",
    "    x = Dense(512, activation='relu', use_bias=False, kernel_regularizer=regularizers.L2(l2=1e-4), name='D_2')(x)\n",
    "    embed = Dense(300, activation='relu', name='embed', use_bias=False, kernel_regularizer=regularizers.L2(l2=1e-4))(x)\n",
    "    outputs = AMSoftmax(1000, s=35, m=0.63, kernel_regularizer=regularizers.L2(l2=1e-4), name='AMSoftmax')(embed)\n",
    "\n",
    "    model = Model(input_1, outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Example TimeDistribute\n",
    "# inputs = tf.keras.Input(shape=(10, 128, 128, 3))\n",
    "# conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))\n",
    "# outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)\n",
    "# outputs.shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Callbaxk**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "sv_mod = ModelCheckpoint(\n",
    "    filepath = 'D:\\\\Andrey\\\\data\\\\weights\\\\ARC_mffc_img\\\\\\\n",
    "model_TDL\\\\modelTD_b128_301_23_0f_ohe_{epoch:02d}-loss-{loss:.4f}_val_los-{val_loss:.4f}.hdf5',\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    save_best_only = True\n",
    ")\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                            patience=6,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.9,\n",
    "                                            min_lr=0.00001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**train**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 409/2276 [====>.........................] - ETA: 6:33 - loss: 7.7719 - accuracy: 0.0022"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_gen\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m              \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_gen\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m              \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43msv_mod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate_reduction\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m              \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Andrey\\tdnn_amsoftmax_speaker_recognition\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\Andrey\\tdnn_amsoftmax_speaker_recognition\\venv\\lib\\site-packages\\keras\\engine\\training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1556\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1558\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1561\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1562\u001B[0m ):\n\u001B[0;32m   1563\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1564\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1566\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mD:\\Andrey\\tdnn_amsoftmax_speaker_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\Andrey\\tdnn_amsoftmax_speaker_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mD:\\Andrey\\tdnn_amsoftmax_speaker_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32mD:\\Andrey\\tdnn_amsoftmax_speaker_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Andrey\\tdnn_amsoftmax_speaker_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mD:\\Andrey\\tdnn_amsoftmax_speaker_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mD:\\Andrey\\tdnn_amsoftmax_speaker_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen,\n",
    "              epochs = 100,\n",
    "              validation_data = (val_gen),\n",
    "              callbacks = [sv_mod, learning_rate_reduction],\n",
    "              verbose = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#model.save('D:\\Andrey\\data\\weights\\ARC_mffc_img\\model_TDL\\Model_weights.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model.load_weights('D:\\Andrey\\data\\weights\\ARC_mffc_img\\model_TDL\\Model_weights.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
